# USF Library Ogden Collection resources

Repository to contain annotation workflow code, basic databases such as controlled vocabulary, and perhaps draft workflows, instructions, or data presentation products (like R Shiny apps) - any of which might later be developed into libguides or use other USF Library-specific resources.

If you're new to GitHub, coding, or data management, feel free to look at the resources I've collected in my lab's general repository: https://github.com/McMinds-Lab/analysis_templates

Example prompts file is just a start. 
- Each row corresponds to a line of questioning
- Each column is separated by a tab character
- The first column of the prompts file has the names of the columns in the final product
  - special keys `point_coordinates` or `box_coordinates` signal the input of multiple sub-observations which can be annotated individually after interactive coordinate selection. These keywords will generate three columns: `point_or_box`, `observation_type` (which is filled automatically with the value in column 2), and `coordinates` (to specify the location within the photo)
  - special format `name:rank1,rank2...` uses `rank1`, `rank2`, etc as column names and signals a taxonomy traversal line of questions with a file that will be loaded into `name`
- The second column of the prompts file has the controlled vocabulary allowed for that annotation
  - or a list of taxa whose ancestors and descendants are controlled with an auxiliary taxonomy file and the keys in column 1 (you could have `taxonomy:order` in column 1 and `Siderastrea siderea` in column 2, and the only possible answer would be `Scleractinia`, *not* `Siderastrea siderea`, unless `species` were also specified in column 1. You could also use `taxonomy:species` in column 1 and `Scleractinia` in column 2, and every species of coral would be available to choose from - but you likely wouldn't want to have such a long list of options if you know most are never going to exist in the photos.
  - note that the ranks will be visited in the order specified, and unambiguous answers automatically filled - so for corals, you might be able to ID 75% to genus but only 15% to species; it might make sense to use `taxonomy:genus,species,family,order,etc` because in 75% of cases you can click twice and be done - ID the genus and then specify that species is ambiguous. If you did `species,genus` it could take just one click 15% of the time and two clicks 75% of the time, but the first drop-down menu may be unwieldy and slow you down via scrolling and hunting on your way to the most common answer - unidentifiable.
  - the special value `unstructured_text` can be used to allow freeform notes at the sub-observation level (notes about the photo as a whole can be taken at any time in the app)
- The third column of the prompts file has the exact prompt that will be displayed in the GUI
  - for auxiliary file traversal, the prompt can make use of the specific key (such as `rank1`) by replacing `{key}` in the text in this field (e.g. `To what {key} does this belong` will be presented as `To what genus does this belong`
- The fourth column of the prompts file has 'dependencies' - these are used to tell the gui to skip the question if it is irrelevant (e.g. there's no point asking what the coral species are in the photo, if we have already determined that there are no corals at all, so `order:Scleractinia` would work if that was annotated earlier).
  - A special keyword `any` can be used to allow any value; for instance `order:any` would trigger this annotation as long as any value has been saved for the key `order` (eg this would be skipped for taxonomic annotations specified as `taxonomy:phylum,class` or for observations that aren't associated with a biological taxonomy, like point_coordinates for all the people in the photo
  - multiple dependencies can be specified with a `;`; eg `order:Scleractinia;healthy:false` for a prompt about bleaching only for unhealthy corals, but allowing fish to also be marked as unhealthy without asking this irrelevant followup.
  - multiple acceptable values within a single annotation column can be specified by separating with commas, eg `class:Hexacorallia,Hydrozoa;healthy:false` to allow both nice corals and angry corals to be annotated as bleached
  - No `OR` operator can be used at the moment for dependencies on different output columns, but the same prompt can be listed twice with different dependencies to trigger it for either case
  - The app will automatically fill in values that have only one option, which you can use with dependencies to speed up annotation conditionally. For instance, if you've clicked on a bunch of paraphyletic benthic invertebrates that are hard to tell apart as a point_coordinates step, you can have the first followup prompt with key `taxonomy:phylum`, values `Porifera,Scleractinia,Chordata`, and dependency `observation_type:benthic_invert`, then another followup with key `taxonomy:genus,species`, values `Siderastrea,Porites,Xestospongia` and dependency `phylum:Cnidaria,Porifera`, then another with key `taxonomy:class`, value `Ascidiacea`, and dependency `phylum:Chordata`. If you annotated a coral, it will prompt for both genus and then species. If you annotated a sponge, it will automatically fill in `Xestospongia` and then ask for species. If you annotated a sea squirt, then the first click identifying it as a chordate is enough for the app to automatically fill in the Class `Ascidiacea`. Despite having its own line in the prompts file, you would never be asked about `Ascidiacea`, but you would always get the more precise annotation automatically. This is useful if you don't even want to try to ID sea squirts more specifically than Class, but you do want to do so for the other taxa that were identified as benthic invertebrates.
- Ideally, we should be able to find wider standards for controlled vocabulary rather than invent our own (e.g. https://github.com/gbif/vocabulary)

Prompts will be visited in the order they are written in the file.

The custom photo annotator can be used by a couple methods. It might not be as good as some polished software, but is nicely customizeable - it's worth looking at various other tools and maybe comparing this to them. To play around with it, there are example 'vocabulary' files and photos included. The easiest are:
1. Use the version that's been exported to https://thecnidaegritty.org/photo_annotator . This won't necessarily be up-to-date with the code maintained here (and currently has a bug for annotations of bounding boxes or point coordinates, so the example prompts file doesn't work and it will only work for whole-photo annotations when you use your own file)
2. Copy and paste the command `shiny::runGitHub(repo = 'McMinds-Lab/USF_Library_Ogden_Collection_resources', ref = 'main', subdir = 'shiny_annotation')` in an R interpreter with the Shiny package installed should work.
3. Download this repo (download the .zip file with the big green `code` button or use `git clone git@github.com:McMinds-Lab/USF_Library_Ogden_Collection_resources.git` in a terminal) and then use the command `shiny::runApp('path/to/USF_Library_Ogden_Collection_resources/shiny_annotation')`
